{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Matrix? Matrix is a way of writing similar things together to handle and manipulate them as per our requirements easily. In Data Science, it is generally used to store information like weights in an Artificial Neural Network while training various algorithms and feature values of a dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "SVD is used to remove the redundant features present in a data set. Suppose if we have a data set which comprises of 1000 features. Definitely, any real data set with such a large number of features is bound to contain redundancy in features. If we need to run Machine Learning algorithm on this dataset we need to be aware of the fact that redundant feature cause a lot of problems while running Machine Learning algorithms. Also, running ML algorithms on such large dataset results in inefficiency and will require a lot of memory.\n",
    "\n",
    "***TruncatedSVD*** from **_sklearn_** used to implements a variant of singular value decomposition (SVD) that only computes the ***k*** largest singular values, where ***k*** is a user-specified parameter.\n",
    "\n",
    "Mathematically, truncated SVD applied to training samples ***X*** produces a low-rank approximation of ***X***. TruncatedSVD is very similar to PCA, but differs in that it works on sample matrices  directly instead of their covariance matrices. When the columnwise (per-feature) means of ***X*** are subtracted from the feature values, truncated SVD on the resulting matrix is equivalent to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.random((5, 8))\n",
    "print(\"Input Matrix is:\")\n",
    "print(X.round(2))\n",
    "\n",
    "svd = TruncatedSVD(n_components=3)\n",
    "svd.fit(X)\n",
    "\n",
    "print()\n",
    "print(\"Decomposed Singular Values are:\",svd.singular_values_.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Drichlet Allocation\n",
    "\n",
    "***Linear Discriminant Analysis (LDA)*** tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.\n",
    "\n",
    "Let us consider Iris dataset for reference: Iris dataset represents 3 kind of Iris flowers _(Setosa, Versicolour and Virginica)_ with 4 attributes: sepal length, sepal width, petal length and petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "lda_X = lda.fit(X, y).transform(X)\n",
    "\n",
    "colors = ['navy', 'green', 'darkorange']\n",
    "lw = 2\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(lda_X[y == i, 0], lda_X[y == i, 1], alpha=.8, color=color,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('LDA of IRIS dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a good understanding of feature selection/ranking can be a great asset for a data scientist or machine learning practitioner.There are in general two reasons why feature selection is used:\n",
    "1. Reducing the number of features, to reduce overfitting and improve the generalization of models.\n",
    "2. To gain a better understanding of the features and their relationship to the response variables.\n",
    "\n",
    "### Univariate feature selection\n",
    "Univariate feature selection examines each feature individually to determine the strength of the relationship of the feature with the response variable. Let us consider Iris data once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest ## for univariate feature selection\n",
    "from sklearn.feature_selection import chi2 \n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "print(\"Old dataset dimensions are:\",X.shape)\n",
    "\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "print(\"New dataset dimensions are:\",X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "Consider a Corpus _C_ of _D_ documents _{d1,d2…..dD}_ and _N_ unique tokens extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix _M_ will be given by ***D X N***. Each row in the matrix _M_ contains the frequency of tokens in document ***D(i)***.\n",
    "\n",
    "Let us understand this using a simple example of four Documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "     'This is about Capillary',\n",
    "     'This is about tfIdf',\n",
    "     'This is about sklearn Tutorial',\n",
    "     \"I'm from Gurgaon\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "countDF = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names(),index=['doc1','doc2','doc3','doc4'])\n",
    "countDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "This is another method which is based on the frequency method but it is different from count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus. Common words like ‘is’, ‘the’, ‘a’ etc. tend to appear quite frequently in comparison to the words which are important to a document.Ideally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents.\n",
    "\n",
    "***TF = (Number of times term t appears in a document)/(Number of terms in the document)***\n",
    "\n",
    "***IDF = log(N/n)***, _where, N is the number of documents and n is the number of documents a term t has appeared in._\n",
    "\n",
    "***TF-IDF = TF X IDF***\n",
    "\n",
    "Let us consider an example of 4 documents below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "     'This is about Capillary',\n",
    "     'This is about tfIdf',\n",
    "     'This is about Tutorial',\n",
    "     \"I'm from Gurgaon\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tfidfDF = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names(),index=['doc1','doc2','doc3','doc4'])\n",
    "tfidfDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook is prepared by _:Rishabh Ojha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
