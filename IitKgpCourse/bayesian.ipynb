{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we will do\n",
    "1. **Bayesian Classifier**\n",
    "    * Refresher of Bayesian theorem\n",
    "    * Build a hand-calculated Bayesian Filter\n",
    "    * Building a email spam filter from scratch using naive bayesian classifier\n",
    "    * Classifier performance\n",
    "    * Recode the same classifier using scikit-learn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Baysian Classifier\n",
    "> It is a type of **probabilistic classifier** that computes **probabilities of each attribute of data belonging to each class** so that it can make prediction of **probability distribution of all classes**.\n",
    "\n",
    "### Why Naive\n",
    "> Due to the assumption that the data attributes are mutually independent. This may sound not completely correct **but it works!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem \n",
    "Let A and B are two events. They can be for example, it will rain today, a person will catch a bus etc. Bayes theorem states that:\n",
    "\n",
    "$P(A|B) = P(B|A)P(A)/P(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us have some fun !!\n",
    "In a factory producing integrated circuits, machine **A** produces **35%**, machine **B** produces **20%** and machine **C** produces **45%** of the products. The fraction of the defective product for machines **A, B and C are 1.5%, 1% and 2% respectively**. An integrated circuit produced by this factory was identified as defective. What are the probabilities that this integrated circuit was manufactured by machines A, B and C respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say the event of an integreated circuit being defective is termed as **D**\n",
    "\n",
    "From Bayes theorem, we get: \n",
    "> P(A|D) = P(D|A)P(A) /  P(D|A)P(A) + P(D|B)P(B) + P(D|C)P(C) = (0.015 \\* 0.35) / (0.015 \\* 0.35)+(0.01 \\* 0.20)+(0.02 \\* 0.45) = 0.323\n",
    "    \n",
    "> P(B|D) = P(D|B)P(B) / P(D|A)P(A) + P(D|B)P(B)+ P(D|C)P(C) = (0.01 \\* 0.20) / (0.015 \\* 0.35)+(0.01 \\* 0.20)+(0.02 \\* 0.45) = 0.123\n",
    "    \n",
    "> P(C|D) = P(D|C)P(C) / P(D|A)P(A) + P(D|B)P(B)+ P(D|C)P(C) = (0.02 \\* 0.45) / (0.015 \\* 0.35)+(0.01 \\* 0.20)+(0.02 \\* 0.45) = 0.554\n",
    "    \n",
    "So, the *probability* of a defective integrated circuit that was manufactured by machines *A, B and C* are **0.323, 0.123 and 0.554** respetively. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let us hand-calculate a bayesian filter\n",
    "### First, establish the conceptual building blocks \n",
    "Given a data sample *X* with *n* features, *X* represents a vector where *X* = (x*1*, x*2*, x*3*, ......., x*n*)\n",
    "\n",
    "The goal of naive bayes is to determine the probabilities that this sample belongs to *K* possible classes. If the classes are y*1*, y*2*, y*3*, ...., y*k*, where k = 1, 2, 3, ...., the goals of naive bayes is to determine \n",
    "> P(y*k*|*X*)\n",
    "\n",
    "So, applying bayes theorem, we get: \n",
    "> P(y*k*|*X*) = P(*X*|y*k*)P(y*k*) / P(*X*)\n",
    "\n",
    "P(y*k*) shows the class distribution and provides no other knoweldge of observed features. This is termed as **'Priori'**. The value of it can be *predetermined*, usually from a uniformly distributed manner where each class has an equal proability of occurance or it can be *learned* from examples.\n",
    "\n",
    "P(y*k*|*X*) provides extra knowledge based on observation and hence termed as **Posteriori**.\n",
    "\n",
    "P(*X*|y*k*) or, P(x*1*, x*2*, x*3*,.......x*n*|y*k*) is a joint distribution of *n* features given the sample belongs to class y*k*. It means how likely the features can co-exist and hence termed as **likelihood**.\n",
    "\n",
    "> The challenge here is that with large number of features, the calculation becomes very complex. This is where the *'naive'* assumptions comes to help. Because *'naive'* assumes that features can occur independtly, the joint conditional distribution of *n* events becomes joint product of individual conditional distributions. Which means:\n",
    "\n",
    "> P(*X*|y*k*) = P(x*1*|y*k*) * P(x*2*|y*k*) * P(x*3*|y*k*) * ......... * P(x*n*|y*k*)\n",
    "\n",
    "This can be efficiently learned from training samples.\n",
    "\n",
    "P(*X*) completely depends upon the distribution of features and are not specific to certain class. Hence, it is termed as **Evidence**. As a result, posterior is proportional to the prior and likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us take an example and apply bayesian filter.\n",
    "\n",
    "Given four emails (just a paper example), the following feature information has been extracted. Our job is to predict how likely a new email will be classified as spam.\n",
    "\n",
    "##### Training Data\n",
    "ID   | Terms in email | Is Spam  \n",
    "--- | --- | --- |\n",
    "1   | Click win prize | Yes \n",
    "2  | Click meeting setup meeting | No \n",
    "3  | Prize free prize | Yes  \n",
    "4  | Click prize free | Yes  \n",
    "\n",
    "\n",
    "##### Test Data\n",
    "ID   | Terms in email | Is Spam  \n",
    "--- | --- | --- |\n",
    "5   | Free setup meeting free | ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define,\n",
    "> **S** = Event that denotes email is spam\n",
    "\n",
    "> **NS** = Event that denotes email is ham\n",
    "\n",
    "Therefore, \n",
    "> P(S) = 3/4\n",
    "\n",
    "> P(NS) = 1/4\n",
    "\n",
    "Now, we have to calculate, \n",
    "> P(S|X), where, X = ('free','setup', 'meeting')\n",
    "\n",
    "To do that, we have to compute:\n",
    "> P(free|S), P(setup|S), P(meeting|S)\n",
    "\n",
    "That is ratio of of occurance of a term to total number of terms in that class.\n",
    "\n",
    "However, the term 'free' does not appear once in class NS. So, the P(free|NS) will become **0**. And as a result, both P(*X*|NS) and P(NS|*X*) will become **0** and the classifier will wrongly classify a ham as a spam!\n",
    "\n",
    "To avoid such zero multiplication factor, each unseen term is intialized to a term frequency value of **1**, i.e. start counting term frequency from **1**. This method is known as *Laplace Smoothing*.\n",
    "\n",
    "Now let us calculate the following:\n",
    "> P(free|S) = (2 + 1) / (9 + 6) = 3 / 15\n",
    "\n",
    "> P(free|NS) = (0 + 1) / (4 + 6) = 1 / 10\n",
    "\n",
    "> P(setup|S) = (0 + 1) / (9 + 6) = 1 / 15\n",
    "\n",
    "> P(setup|NS) = (1 + 1) / (4 + 6) = 2 / 10 = 1 / 5\n",
    "\n",
    "> P(meeting|S) = (0 + 1) / (9 + 6) = 1 / 15\n",
    "\n",
    "> P(meeting|NS) = (2 + 1) / (4 + 6) = 3 / 10\n",
    "\n",
    "Therefore, the probability of a the test email falling in a class can be calculated by \n",
    "* extracting terms from the email\n",
    "* calculate the ratio of tje terms joint probability to fall under S or NS\n",
    "* calcuate the probability for each each class\n",
    "\n",
    "The test email has four terms: free, setup, meeting, free. Let us calculate:\n",
    "\n",
    "> P(S|*X*) / P(NS|*X*) = P(free|S)P(setup|S)P(meeting|S)P(free|S)P(S) / P(free|NS)P(setup|NS)P(meeting|NS)P(free|NS)P(NS)\n",
    "\n",
    "> = 8 / 9\n",
    "\n",
    "So, the probability of the test email being a spam is:\n",
    "> 8 / 8 + 9 = 8 / 17 = **47.1%**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Naive Bayes email spam filter\n",
    "Now, let us write code to implement a working bayesian filter for email spam filtering. We will use a real-life data set taken from Enron email dataset which is available for download at: http://aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz. \n",
    "\n",
    "The summary statistics of the data is as follows:\n",
    "\n",
    "#### Legitimate\n",
    "- Owner: farmer-d\n",
    "- Total number: 3672 emails\n",
    "- Date of first email: 1999-12-10\n",
    "- Date of last email: 2002-01-11\n",
    "- Similars deletion: No\n",
    "- Encoding: No\n",
    "\n",
    "#### Spam\n",
    "- Owner: GP\n",
    "- Total number: 1500 emails\n",
    "- Date of first email: 2003-12-18\n",
    "- Date of last email: 2005-09-06\n",
    "- Similars deletion: No\n",
    "- Encoding: No\n",
    "\n",
    "*Spam:Legitimate rate* = **1:3**\n",
    "*Total number of emails (legitimate + spam)*: **5975**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the filter in steps.\n",
    "\n",
    "** So, first let us see how the spam and ham emails look:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_file_path = 'data/email/enron1/ham/2542.2000-10-16.farmer.ham.txt'\n",
    "\n",
    "with open(ham_file_path, 'r') as in_file:\n",
    "    ham_sample = in_file.read()\n",
    "\n",
    "print(ham_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_file_path = 'data/email/enron1/spam/5143.2005-09-04.GP.spam.txt'\n",
    "\n",
    "with open(spam_file_path, 'r') as in_file:\n",
    "    spam_sample = in_file.read()\n",
    "\n",
    "print(spam_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we read all ham and spam data and store the label information in variables. A spam class is represented by 1 and a ham class is represented by 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "e_mails, labels = [], []\n",
    "\n",
    "spam_directory = 'data/email/enron1/spam'\n",
    "ham_directory = 'data/email/enron1/ham'\n",
    "\n",
    "for file_name in glob.glob(os.path.join(spam_directory, '*.txt')):\n",
    "    with open(file_name, 'r', encoding = 'ISO-8859-1') as in_file:\n",
    "        e_mails.append(in_file.read())\n",
    "        labels.append(1)\n",
    "        \n",
    "for file_name in glob.glob(os.path.join(ham_directory, '*.txt')):\n",
    "    with open(file_name, 'r', encoding = 'ISO-8859-1') as in_file:\n",
    "        e_mails.append(in_file.read())\n",
    "        labels.append(0)\n",
    "\n",
    "print(len(e_mails))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The next step is to preprocess email text. This will include:**\n",
    "* removal of numbers and punctuation\n",
    "* human name removal\n",
    "* stop words removal\n",
    "* lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"names\")\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ********************************\n",
    "def is_letters_only(astr):\n",
    "    return astr.isalpha()\n",
    "\n",
    "all_names = set(names.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ********************************\n",
    "def clean_text(docs):\n",
    "    cleaned_docs = []\n",
    "    for doc in docs:\n",
    "        cleaned_docs.append(' '.join([lemmatizer.lemmatize(word.lower())\n",
    "                                        for word in doc.split()\n",
    "                                        if is_letters_only(word)\n",
    "                                        and word not in all_names]))\n",
    "    return cleaned_docs\n",
    "\n",
    "# ********************************\n",
    "cleaned_emails = clean_text(e_mails)\n",
    "term_docs = cv.fit_transform(cleaned_emails)\n",
    "print(term_docs [0])\n",
    "\n",
    "feature_mapping = cv.vocabulary\n",
    "feature_names = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we need to remove stop words and extract important features which are term frequencies. The CountVectorizer class of sklearn library is used to do the work. Here we limit to extract 500 most frequent terms. However, we can tewak this parameter for tuning accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', max_features=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizer turns the document matrix into a term-document matrix where each row is a term frequency sparse vector in the form of *(row index, term index), value*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_docs = cv.fit_transform(cleaned_emails)\n",
    "\n",
    "print(term_docs.shape[0])\n",
    "print(term_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the corrosponding terms by using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "print(feature_names[491])\n",
    "print(feature_names[197])\n",
    "print(feature_names[445])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With the feature matrix term_docs generated, we can now build and start training our bayes model.**\n",
    "* First we group the priori data by label\n",
    "\n",
    "This will group training sample indices by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_label_index(labels):\n",
    "    label_index = defaultdict(list)\n",
    "    \n",
    "    for index, label in enumerate(labels):\n",
    "        label_index[label].append(index)\n",
    "    return label_index\n",
    "\n",
    "# ********************************\n",
    "label_index = get_label_index(labels)\n",
    "print(label_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With this, we calculate prior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior(label_index):\n",
    "    \"\"\" Compute prior based on training samples\n",
    "    Args:\n",
    "        label_index (grouped sample indices by class)\n",
    "    Returns:\n",
    "        dictionary, with class label as key, corresponding prior as the value\n",
    "    \"\"\"\n",
    "    prior = {label: len(index) for label, index in label_index.items()}\n",
    "    total_count = sum(prior.values())\n",
    "    \n",
    "    for label in prior:\n",
    "        prior[label] /= float(total_count)\n",
    "    return prior\n",
    "\n",
    "# ********************************\n",
    "prior = get_prior(label_index)\n",
    "print(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_likelihood(term_document_matrix, label_index, smoothing=0):\n",
    "    \"\"\" Compute likelihood based on training samples\n",
    "    Args:\n",
    "        term_document_matrix (sparse matrix)\n",
    "        label_index (grouped sample indices by class)\n",
    "        smoothing (integer, additive Laplace smoothing parameter)\n",
    "    Returns:\n",
    "        dictionary, with class as key, corresponding conditional probability P(feature|class) vector as value\n",
    "    \"\"\"\n",
    "    likelihood = {}\n",
    "    for label, index in label_index.items():\n",
    "        likelihood[label] = term_document_matrix[index, :].sum(axis=0) + smoothing\n",
    "        \n",
    "        likelihood[label] = np.asarray(likelihood[label])[0]\n",
    "        \n",
    "        total_count = likelihood[label].sum()\n",
    "        \n",
    "        likelihood[label] = likelihood[label] / float(total_count)\n",
    "    return likelihood\n",
    "\n",
    "# ********************************\n",
    "smoothing = 1\n",
    "likelihood = get_likelihood(term_docs, label_index, smoothing)\n",
    "\n",
    "print('Number of features: ', len(likelihood[0]))\n",
    "print('First five element of P(feature|ham): ', likelihood[0][:5])\n",
    "print('First five element of P(feature|spam): ', likelihood[1][:5])\n",
    "print('Feature names: ', feature_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With Prior and Likelihood ready, we can now calculate Posteriori for test samples and new emails**\n",
    "> Instead of multiplying hundreds and thousands of small value conditional probability which will cause numerical overflow, we will convert multiplication into natural logarithmic sum and convert back to its natural exponential value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(term_document_matrix, prior, likelihood):\n",
    "    \"\"\"\n",
    "        Compute posteriori of testing samples based on priori and likelihood\n",
    "        \n",
    "        posteriori is proportional to priori * likelihood\n",
    "        = exp(log(priori * likelihood))\n",
    "        = exp(log(prior) + log(likelihood))\n",
    "        \n",
    "        Args:\n",
    "            term_document_matrix\n",
    "            prior\n",
    "            likelihood\n",
    "            \n",
    "        Returns:\n",
    "            {class label : posterior value}\n",
    "    \"\"\"\n",
    "    num_docs = term_document_matrix.shape[0]\n",
    "    # print(num_docs)\n",
    "    \n",
    "    posteriors = []\n",
    "    \n",
    "    for doc_index in range(num_docs):\n",
    "        posterior = {key : np.log(prior_label) for key, prior_label in prior.items()}\n",
    "        \n",
    "        for label, likelihood_label in likelihood.items():\n",
    "            term_document_vector = term_document_matrix.getrow(doc_index)\n",
    "            \n",
    "            counts = term_document_vector.data\n",
    "            indices = term_document_vector.indices\n",
    "            \n",
    "            for count, index in zip(counts, indices):\n",
    "                posterior[label] += np.log(likelihood_label[index]) * count\n",
    "                \n",
    "            min_log_posterior = min(posterior.values())\n",
    "            \n",
    "            for label in posterior:\n",
    "                try:\n",
    "                    posterior[label] = np.exp(posterior[label] - min_log_posterior)\n",
    "                except:\n",
    "                    # If log value execessively large, assign infinity\n",
    "                    posterior[label] = float('inf')\n",
    "            \n",
    "            # Normalize so that it sums upto 1\n",
    "            sum_posterior = sum(posterior.values())\n",
    "            \n",
    "            for label in posterior:\n",
    "                if posterior[label] == float('inf'):\n",
    "                    posterior[label] = 1.0\n",
    "                else:\n",
    "                    posterior[label] /= sum_posterior\n",
    "                \n",
    "            posteriors.append(posterior.copy())\n",
    "            \n",
    "    return posteriors\n",
    "\n",
    "# ********************************        \n",
    "posteriors = get_posterior(term_docs, prior, likelihood)\n",
    "#print(posteriors) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The prediction function is now complete. Let us now test the classifier with test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_index = get_label_index(labels)\n",
    "prior = get_prior(label_index)\n",
    "\n",
    "smoothing = 1\n",
    "likelihood = get_likelihood(term_docs, label_index, smoothing)\n",
    "\n",
    "emails_test = [\n",
    "    '''Subject: flat screens\n",
    "    hello ,\n",
    "    please call or contact regarding the other flat screens requested .\n",
    "    trisha tlapek - eb 3132 b\n",
    "    michael sergeev - eb 3132 a\n",
    "    also the sun blocker that was taken away from eb 3131 a .\n",
    "    trisha should two monitors also michael .\n",
    "    thanks\n",
    "    kevin moore''',\n",
    "    '''Subject: having problems in bed ? we can help !\n",
    "    cialis allows men to enjoy a fully normal conjugal life without problem.\n",
    "    if we let things terrify us , life will not be worth living .\n",
    "    brevity is the soul of lingerie .\n",
    "    suspicion always haunts the guilty mind .''',\n",
    "]\n",
    "\n",
    "cleaned_test = clean_text(emails_test)\n",
    "term_docs_test = cv.transform(cleaned_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)\n",
    "\n",
    "#print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To comprehensively evaluate our classifier's performance, we need to divide the data set randomly into training and test set. That will simulate learning and testing data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cleaned_emails, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "len(X_train), len(Y_train)\n",
    "len(X_test), len(Y_test)\n",
    "\n",
    "term_docs_train = cv.fit_transform(X_train)\n",
    "label_index = get_label_index(Y_train)\n",
    "prior = get_prior(label_index)\n",
    "likelihood = get_likelihood(term_docs_train, label_index, smoothing)\n",
    "\n",
    "term_docs_test = cv.transform(X_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)\n",
    "\n",
    "correct = 0.0\n",
    "\n",
    "for pred, actual in zip(posterior, Y_test):\n",
    "    if actual == 1:\n",
    "        if pred[1] >= 0.5:\n",
    "            correct += 1\n",
    "    elif pred[0] > 0.5:\n",
    "        correct += 1\n",
    "\n",
    "print('The accuracy on {0} testing samples is: {1:.1f}%'.format(len(Y_test), correct/len(Y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our handcrafted classifier is not performing that well. We need to check computation especially for numerical overflow. The idea was to show what it takes to build a bayesian classifier. \n",
    "\n",
    "Now, we will use sklearn machine learning library gunction to to the same classification on same data set. The following code show that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "clf.fit(term_docs_train, Y_train)\n",
    "prediction_prob = clf.predict_proba(term_docs_test)\n",
    "prediction_prob[0:10]\n",
    "prediction = clf.predict(term_docs_test)\n",
    "prediction[:10]\n",
    "accuracy = clf.score(term_docs_test, Y_test)\n",
    "print('The accuracy using MultinomialNB is: {0:.1f}%'.format(accuracy*100))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, prediction, labels=[0, 1])\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "precision_score(Y_test, prediction, pos_label=1)\n",
    "recall_score(Y_test, prediction, pos_label=1)\n",
    "f1_score(Y_test, prediction, pos_label=1)\n",
    "\n",
    "f1_score(Y_test, prediction, pos_label=0)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test, prediction)\n",
    "print(report)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pos_prob = prediction_prob[:, 1]\n",
    "thresholds = np.arange(0.0, 1.2, 0.1)\n",
    "true_pos, false_pos = [0]*len(thresholds), [0]*len(thresholds)\n",
    "for pred, y in zip(pos_prob, Y_test):\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        if pred >= threshold:\n",
    "            if y == 1:\n",
    "                true_pos[i] += 1\n",
    "            else:\n",
    "                false_pos[i] += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "true_pos_rate = [tp / 516.0 for tp in true_pos]\n",
    "false_pos_rate = [fp / 1191.0 for fp in false_pos]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(false_pos_rate, true_pos_rate, color='darkorange',\n",
    "         lw=lw)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(Y_test, pos_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "k = 10\n",
    "k_fold = StratifiedKFold(n_splits=k)\n",
    "\n",
    "# convert to numpy array for more efficient slicing\n",
    "cleaned_emails_np = np.array(cleaned_emails)\n",
    "labels_np = np.array(labels)\n",
    "\n",
    "max_features_option = [2000, 4000, 8000]\n",
    "smoothing_factor_option = [0.5, 1.0, 1.5, 2.0]\n",
    "fit_prior_option = [True, False]\n",
    "auc_record = {}\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(cleaned_emails, labels):\n",
    "    X_train, X_test = cleaned_emails_np[train_indices], cleaned_emails_np[test_indices]\n",
    "    Y_train, Y_test = labels_np[train_indices], labels_np[test_indices]\n",
    "    \n",
    "    for max_features in max_features_option:\n",
    "        \n",
    "        if max_features not in auc_record:\n",
    "            auc_record[max_features] = {}\n",
    "        \n",
    "        cv = CountVectorizer(stop_words=\"english\", max_features=max_features)\n",
    "        \n",
    "        term_docs_train = cv.fit_transform(X_train)\n",
    "        term_docs_test = cv.transform(X_test)\n",
    "        \n",
    "        for smoothing_factor in smoothing_factor_option:\n",
    "            \n",
    "            if smoothing_factor not in auc_record[max_features]:\n",
    "                auc_record[max_features][smoothing_factor] = {}\n",
    "            \n",
    "            for fit_prior in fit_prior_option:\n",
    "                clf = MultinomialNB(alpha=smoothing_factor, fit_prior=fit_prior)\n",
    "                \n",
    "                clf.fit(term_docs_train, Y_train)\n",
    "                \n",
    "                prediction_prob = clf.predict_proba(term_docs_test)\n",
    "                \n",
    "                pos_prob = prediction_prob[:, 1]\n",
    "                \n",
    "                auc = roc_auc_score(Y_test, pos_prob)\n",
    "                \n",
    "                auc_record[max_features][smoothing_factor][fit_prior] \\\n",
    "                    = auc + auc_record[max_features][smoothing_factor].get(fit_prior, 0.0)\n",
    "\n",
    "print(auc_record)\n",
    "\n",
    "print('max features  smoothing  fit prior  auc')\n",
    "for max_features, max_feature_record in auc_record.items():\n",
    "    for smoothing, smoothing_record in max_feature_record.items():\n",
    "        for fit_prior, auc in smoothing_record.items():\n",
    "            print('       {0}      {1}      {2}    {3:.4f}'.format(max_features, smoothing, fit_prior, auc/k))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
